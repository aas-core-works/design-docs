{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Design Docs for aasx-core This collection of documents captures the design decisions we made while discussing and designing the aasx-core library. A Note about Validity Their first and foremost aim is to kick-start the development and guide the initial implementation. The documents will not be continuously updated once we start with the development. However, they are still useful for the future developers! For example, parts of the code might be clearer when the initial design decisions are considered.","title":"Home"},{"location":"#design-docs-for-aasx-core","text":"This collection of documents captures the design decisions we made while discussing and designing the aasx-core library.","title":"Design Docs for aasx-core"},{"location":"#a-note-about-validity","text":"Their first and foremost aim is to kick-start the development and guide the initial implementation. The documents will not be continuously updated once we start with the development. However, they are still useful for the future developers! For example, parts of the code might be clearer when the initial design decisions are considered.","title":"A Note about Validity"},{"location":"contracts/","text":"Code Contracts The specification of operations in our meta-model should include the code contracts such as pre-conditions and post-conditions. The contracts can be trivial (such as specifying a range of a property), as well as very complex expressions (the length of this list needs to be divisble by three and the first element needs to be larger than the sum of all the other elements). The code generators transpile contracts so that they are included in the implementation . This has the following benefits: * The implementation of the library allows thus for deeper testing as contracts are checked during debuging, testing and in production. Additionally, the implementation can be analyzed with static analyzers and automatic testers (such as crosshair and icontract-hypothesis ), which can only work with code with tight-enough contracts. The contracts are included in the documentation of the implementation . Hence the users can rely on formal unambiguous documentation. Moreover, the documentation in human language tends to \"rot\", while contracts are continuously and automatically verified in testing or in production. Definition of Contracts The contracts are written in SHACL and transpiled by code generators into the implementation stubs.. As we could not support invariants in most languages, we decide to define contracts as pre-conditions and post-conditions of operations . Each contract should be given a unique identifier . Error Messages The contracts should also include an error message . The error message should support printf format strings so that we can include the actual offending values in the message as well. This is important for contracts which are enforced in production catching rare and hard-to-reproduce bugs . Severity Each contract is given a severity which determines when it is enforced: * ALWAYS , always enforced, including in production, * DEBUG , enforced both in our and external tests, and * DEBUG_OURS , enforced only in our tests, but excluded in external tests, * DEBUG_SLOW and DEBUG_OURS_SLOW , enforced only in the respective tests for which we know that the amount data is small. This allows us to enforce contracts with exploding computational complexity. For example, most post-conditions have the severity DEBUG . Contracts and Serialization The contracts are a tool for improving the correctness of the code. They are not meant to be used as validation of the input ( e.g. , a JSON file to be imported). In our setting, this means that the validation of the input is part of the deserialization scripts . They need to check that the input is correct and conforms to the meta-model, and report the errors eventually to the user. In contrast, the contracts give us a certain assurance that the de-serialized objects are internally consistent and conform to our meta-model. In other words, the deserialization scripts verify the external data , while contracts verify the internal data . For example, we can use contracts to ensure that our deserialization works. Instead of writing tons of unit tests, we rely on the contracts to be verified during testing. A test boils down to simply de-serializing the example JSONs.","title":"Code Contracts"},{"location":"contracts/#code-contracts","text":"The specification of operations in our meta-model should include the code contracts such as pre-conditions and post-conditions. The contracts can be trivial (such as specifying a range of a property), as well as very complex expressions (the length of this list needs to be divisble by three and the first element needs to be larger than the sum of all the other elements). The code generators transpile contracts so that they are included in the implementation . This has the following benefits: * The implementation of the library allows thus for deeper testing as contracts are checked during debuging, testing and in production. Additionally, the implementation can be analyzed with static analyzers and automatic testers (such as crosshair and icontract-hypothesis ), which can only work with code with tight-enough contracts. The contracts are included in the documentation of the implementation . Hence the users can rely on formal unambiguous documentation. Moreover, the documentation in human language tends to \"rot\", while contracts are continuously and automatically verified in testing or in production.","title":"Code Contracts"},{"location":"contracts/#definition-of-contracts","text":"The contracts are written in SHACL and transpiled by code generators into the implementation stubs.. As we could not support invariants in most languages, we decide to define contracts as pre-conditions and post-conditions of operations . Each contract should be given a unique identifier .","title":"Definition of Contracts"},{"location":"contracts/#error-messages","text":"The contracts should also include an error message . The error message should support printf format strings so that we can include the actual offending values in the message as well. This is important for contracts which are enforced in production catching rare and hard-to-reproduce bugs .","title":"Error Messages"},{"location":"contracts/#severity","text":"Each contract is given a severity which determines when it is enforced: * ALWAYS , always enforced, including in production, * DEBUG , enforced both in our and external tests, and * DEBUG_OURS , enforced only in our tests, but excluded in external tests, * DEBUG_SLOW and DEBUG_OURS_SLOW , enforced only in the respective tests for which we know that the amount data is small. This allows us to enforce contracts with exploding computational complexity. For example, most post-conditions have the severity DEBUG .","title":"Severity"},{"location":"contracts/#contracts-and-serialization","text":"The contracts are a tool for improving the correctness of the code. They are not meant to be used as validation of the input ( e.g. , a JSON file to be imported). In our setting, this means that the validation of the input is part of the deserialization scripts . They need to check that the input is correct and conforms to the meta-model, and report the errors eventually to the user. In contrast, the contracts give us a certain assurance that the de-serialized objects are internally consistent and conform to our meta-model. In other words, the deserialization scripts verify the external data , while contracts verify the internal data . For example, we can use contracts to ensure that our deserialization works. Instead of writing tons of unit tests, we rely on the contracts to be verified during testing. A test boils down to simply de-serializing the example JSONs.","title":"Contracts and Serialization"},{"location":"data-structures-and-operations/","text":"Operations TODO (Nico, 2021-03-26): Write","title":"Data Structures and Operations"},{"location":"data-structures-and-operations/#operations","text":"TODO (Nico, 2021-03-26): Write","title":"Operations"},{"location":"deserialization-scripts/","text":"De/serialization Scripts For many similar languages following the procedural paradigm, it is tedious to write the deserialization over and over again. For example, once we implemented the deserialization of JSON in C#, it will be most probably the same thing in Java, and very similar in Golang. ( 2021-03-26: Functional languages are an exception here, but that is a problem we abstract away at the moment. ) Instead, we propose to specify the de-serialization and serialization in a meta-language as a de/serialization script . A de/serialization script is then transpiled by the code generators into the respective implementation of the library. Scripting Language We use a subset of Python language to script the de/serialization. As the de/serialization scripts are not used by the wide audience, but only by the developers of the library, we intentionally decide on-the-go which Python features to support. Hence we only transpile a very limited subset of builtin functions and types. For example, we allow no nested functions, classes or context managers. Operations Each de-serialization and serialization operation is given as a function . The function operates on a jsonable , the corresponding object to which the parsed data should be added and a path to the JSON-able. For example parse_submodel_element : def parse_submodel_element( jsonable: object, submodel: Submodel, path: pathlib.Path) -> None: ... Provided Commands and Queries It is assumed that the de/serialization script can access the operations and the data structures of our library (such as Submodel in the aforementioned example). Additionally, special commands and queries such as is_int32 , is_int64 , \u2026, as_int32 , as_int64 , as_float32 , as_float64 , as_list and as_map are provided to implement the logic. Functions such as x = local_int32(0) are used to instantiate local variables. These special commands and queries are implemented in a separate module, say, aasx_core_gen.de_serialization . Supported Language Constructs For-loops can loop either over lists, over maps or over for _ in range(..., ...) where both start and end are given as integers. Error Messages The errors should be reported using pre-defined functions error(message: str, path: pathlib.Path) . The messages should use old string formatting to allow for easier transpilation to languages such as C++ and Golang. Include Markers Since we only provide a transpilation based on a subset of Python language, many operations will have to include manually defined code snippets. The developer should mark such spots with include markers using NotImplementedError . For example: raise NotImplementedError('some-marker') will make the code generators introduce an include marker some-marker in the generated code. The filler script will eventually replace the include markers with the corresponding code snippets written in the implementation language. Relation to JSON Schema and XML Schema Definition We should write a separate code generator to bootstrap this scripting code based on a schema ( e.g. , a JSON Schema ), but the schema should not be referenced in the meta-model nor available directly in code. There are two reasons for this decision: Validation and parsing of the input needs complex logic . We need to support dialects or instantiate different objects based on a complex logic. This is not possible to define in a schema. The generated implementation does not depend on a schema. Few languages support JSON Schema and XML Schema Definition out-of-box. The schema libraries, if available, tend to be slow and buggy (in general). When the \"official\" schema changes, we need to manually update our code. For example, we could use the bootstrapping code generator to re-generate the code based on the new schema and inspect the changes with the diff tool. This is clearly tedious, but we found no easy way around it. TODO (mristin, 2021-03-26): Draw a diagram to show how the workflow looks like Implementation Considerations Error logging Error logging is more involved than it seems at first. We probably need to cap the maximum number of errors \u2014 once the error log is full, the parsing should be immediately stopped. This can be implemented by passing an ErrorLog to parse_* functions. As error logs are capped, the generated code should automatically check for available capacity at certain points (e.g., before descending into parsing the child elements), and abort the parsing if the capacity has been reached. This might also have implications on efficiency \u2014 but so do exceptions and stack unwinding! We might consider making two variants of parse_* functions \u2014 one with ErrorLog and one without (which raises the exceptions). The latter, which raises the exceptions, uses the first variant and passes an ErrorRaiser (or ErrorThrower , depending on the language) as ErrorLog . Bugs with Floats and Integers JSON specifies that all numbers (both integers and floats) are given as 64-bit floating point numbers (see this comment on a GitHub issue ). Most JSON libraries will thus handle all numbers as 64-bit floats. This can lead to hard-to-spot bugs when the files are deserialized. Here is a Python snippet to illustrate the unexpected behavior: import json # Mind the last digit before the comma! >>> json.loads('9007199254740993.0') 9007199254740992.0 Similarly, in Golang: package main import ( \"encoding/json\" \"fmt\" ) func main() { var x interface{} json.Unmarshal([]byte(\"9007199254740993\"), &x) xFloat64 := x.(float64) fmt.Printf(\"x is %f\\n\", xFloat64) } will give you: x is 9007199254740992.000000 (again, mind the last digit). While we can not fix the de-serialization, as we usually rely on the built-in library, we could at least raise an exception during the serialization. For example, if we are serializing an 64-bit integer to JSON, we need to ensure that it is less-equal 2^52 (see again this comment on a GitHub issue ). While the issue seems rare and irrelevant, that is not the case in the practice. All data structures that involve unique identifiers given as 64-bit integer numbers will exhibit the bug if they do not increment the identifiers consecutively, but generate random 64-bit identifiers. Functional languages We probably don't need a different scripting language for purely functional languages, but we need to think hard about it. At this stage, let us cover the procedural languages, and then think about how we can generate code for functional languages, and observe only then what walls we hit. Debugging There should be a flag in the code generators to allow for inclusion of the original scripts in the generated code as comments. Thus the generated code is easier to trace and double-check.","title":"De/serialization Scripts"},{"location":"deserialization-scripts/#deserialization-scripts","text":"For many similar languages following the procedural paradigm, it is tedious to write the deserialization over and over again. For example, once we implemented the deserialization of JSON in C#, it will be most probably the same thing in Java, and very similar in Golang. ( 2021-03-26: Functional languages are an exception here, but that is a problem we abstract away at the moment. ) Instead, we propose to specify the de-serialization and serialization in a meta-language as a de/serialization script . A de/serialization script is then transpiled by the code generators into the respective implementation of the library.","title":"De/serialization Scripts"},{"location":"deserialization-scripts/#scripting-language","text":"We use a subset of Python language to script the de/serialization. As the de/serialization scripts are not used by the wide audience, but only by the developers of the library, we intentionally decide on-the-go which Python features to support. Hence we only transpile a very limited subset of builtin functions and types. For example, we allow no nested functions, classes or context managers.","title":"Scripting Language"},{"location":"deserialization-scripts/#operations","text":"Each de-serialization and serialization operation is given as a function . The function operates on a jsonable , the corresponding object to which the parsed data should be added and a path to the JSON-able. For example parse_submodel_element : def parse_submodel_element( jsonable: object, submodel: Submodel, path: pathlib.Path) -> None: ...","title":"Operations"},{"location":"deserialization-scripts/#provided-commands-and-queries","text":"It is assumed that the de/serialization script can access the operations and the data structures of our library (such as Submodel in the aforementioned example). Additionally, special commands and queries such as is_int32 , is_int64 , \u2026, as_int32 , as_int64 , as_float32 , as_float64 , as_list and as_map are provided to implement the logic. Functions such as x = local_int32(0) are used to instantiate local variables. These special commands and queries are implemented in a separate module, say, aasx_core_gen.de_serialization .","title":"Provided Commands and Queries"},{"location":"deserialization-scripts/#supported-language-constructs","text":"For-loops can loop either over lists, over maps or over for _ in range(..., ...) where both start and end are given as integers.","title":"Supported Language Constructs"},{"location":"deserialization-scripts/#error-messages","text":"The errors should be reported using pre-defined functions error(message: str, path: pathlib.Path) . The messages should use old string formatting to allow for easier transpilation to languages such as C++ and Golang.","title":"Error Messages"},{"location":"deserialization-scripts/#include-markers","text":"Since we only provide a transpilation based on a subset of Python language, many operations will have to include manually defined code snippets. The developer should mark such spots with include markers using NotImplementedError . For example: raise NotImplementedError('some-marker') will make the code generators introduce an include marker some-marker in the generated code. The filler script will eventually replace the include markers with the corresponding code snippets written in the implementation language.","title":"Include Markers"},{"location":"deserialization-scripts/#relation-to-json-schema-and-xml-schema-definition","text":"We should write a separate code generator to bootstrap this scripting code based on a schema ( e.g. , a JSON Schema ), but the schema should not be referenced in the meta-model nor available directly in code. There are two reasons for this decision: Validation and parsing of the input needs complex logic . We need to support dialects or instantiate different objects based on a complex logic. This is not possible to define in a schema. The generated implementation does not depend on a schema. Few languages support JSON Schema and XML Schema Definition out-of-box. The schema libraries, if available, tend to be slow and buggy (in general). When the \"official\" schema changes, we need to manually update our code. For example, we could use the bootstrapping code generator to re-generate the code based on the new schema and inspect the changes with the diff tool. This is clearly tedious, but we found no easy way around it. TODO (mristin, 2021-03-26): Draw a diagram to show how the workflow looks like","title":"Relation to JSON Schema and XML Schema Definition"},{"location":"deserialization-scripts/#implementation-considerations","text":"","title":"Implementation Considerations"},{"location":"deserialization-scripts/#error-logging","text":"Error logging is more involved than it seems at first. We probably need to cap the maximum number of errors \u2014 once the error log is full, the parsing should be immediately stopped. This can be implemented by passing an ErrorLog to parse_* functions. As error logs are capped, the generated code should automatically check for available capacity at certain points (e.g., before descending into parsing the child elements), and abort the parsing if the capacity has been reached. This might also have implications on efficiency \u2014 but so do exceptions and stack unwinding! We might consider making two variants of parse_* functions \u2014 one with ErrorLog and one without (which raises the exceptions). The latter, which raises the exceptions, uses the first variant and passes an ErrorRaiser (or ErrorThrower , depending on the language) as ErrorLog .","title":"Error logging"},{"location":"deserialization-scripts/#bugs-with-floats-and-integers","text":"JSON specifies that all numbers (both integers and floats) are given as 64-bit floating point numbers (see this comment on a GitHub issue ). Most JSON libraries will thus handle all numbers as 64-bit floats. This can lead to hard-to-spot bugs when the files are deserialized. Here is a Python snippet to illustrate the unexpected behavior: import json # Mind the last digit before the comma! >>> json.loads('9007199254740993.0') 9007199254740992.0 Similarly, in Golang: package main import ( \"encoding/json\" \"fmt\" ) func main() { var x interface{} json.Unmarshal([]byte(\"9007199254740993\"), &x) xFloat64 := x.(float64) fmt.Printf(\"x is %f\\n\", xFloat64) } will give you: x is 9007199254740992.000000 (again, mind the last digit). While we can not fix the de-serialization, as we usually rely on the built-in library, we could at least raise an exception during the serialization. For example, if we are serializing an 64-bit integer to JSON, we need to ensure that it is less-equal 2^52 (see again this comment on a GitHub issue ). While the issue seems rare and irrelevant, that is not the case in the practice. All data structures that involve unique identifiers given as 64-bit integer numbers will exhibit the bug if they do not increment the identifiers consecutively, but generate random 64-bit identifiers.","title":"Bugs with Floats and Integers"},{"location":"deserialization-scripts/#functional-languages","text":"We probably don't need a different scripting language for purely functional languages, but we need to think hard about it. At this stage, let us cover the procedural languages, and then think about how we can generate code for functional languages, and observe only then what walls we hit.","title":"Functional languages"},{"location":"deserialization-scripts/#debugging","text":"There should be a flag in the code generators to allow for inclusion of the original scripts in the generated code as comments. Thus the generated code is easier to trace and double-check.","title":"Debugging"},{"location":"general-design-decisions/","text":"General Design Decisions Mission We want to provide a library for manipulating the static information of the asset administration shell with a unified interface and implementations in many programming languages. Scope The library covers the data model of the shell and its serialization. We aim to implement the library at the level of quality ready for production . We embrace stability and long-term support instead of experimental features . The library should be minimalistic (as opposed to large enterprise systems). The library should provide base for a rich toolkit built on top of it. For example, generators such as aasx-to-grpc or aasx-to-opc-ua-server. Out-of-Scope The component manager , the \"active\" part of the shell, is out-of-scope. This is the responsability of other components such as aasx-server . The thread-safe operations and ACID transactions are out-of-scope. The locking as well as write-ahead logs are left to the user. A different library, aasx-db (yet to be created), will provide such functionality. We expect the data model of an asset administration shell to fit in memory . Otherwise, components such as aasx-server and aasx-db (yet to be created) should be used. Since query language has not been standardized yet, we will not implement it at this point (March 2021). The library should provide basic quering through dedicated operations based on the existing implementations, such as the data model of the AASX Package Explorer . Requirements The library should be provided in many programming languages . We aim for unified interface shared between the languages. Thus the users which use multiple languages can use the library effortlessly between them, as the interface remains familiar. The main user base are the developers working on stable production systems . However, the library should be flexible enough to allow experimenters to use it in their experimental code bases. Certain patterns should be supported out-of-the-box. For example, code for visitors and transformers should be included. The library should minimize the dependencies to avoid the dependency hell and be runnable on as many systems as possible. The implementations should be in native code (instead of wrapping, say, C++ code). This allows language-specific power features such as reflection. The library should provide read/write operations on AASX files . This means that blobs and packaged files should be readable/writable, but no additional features on the blobs should be made available. The library should provide import/export for JSON and XML . Other formats such as AutomationML and RDF are not supported out-of-the-box. However, adding imports and exports for additional formats should be straightforward, so that users can develop their own import/export libraries on the top. We should explicitly track how much friction there is if the user wants to use RDF, AML or any other serialization format. For example, can the user just load an AASX and use RDF? Is there a hook to set up so that the user can choose which serialization methods should be supported? The generated code should allow for static analysis as much as possible. For example, Nullable types should be used wherever appropriate. We should support wide range of computer architectures . For example, we need to support Mono in C# so that the library can run on ARM32 machines. This is important as embedded systems often run on such machines. We need to support the Environment (multiple shells) , but not the shell repository. You should be able to implement the repository using our library, but our library does not care about the repository. We postpone the discussion about the middlewares . For example, it would be practical if we allowed for a middleware to resolve ecl@ss automatically. However, this is not trivial and we leave this out at the moment (2021-03-26). The user should not use constructors to create objects. We want to keep migrations between aasx-core and aasx-db (its thread-safe pendant based on transactions) as effortless as possible. Since aasx-db is an object-oriented database , it needs to keep track of the objects and their state. Therefore, constructors need to be handled through factory methods to prepare for a future migration to aasx-db. Computational efficiency is important. The library will be used in large batches as well as servers which need to have large throughput. Development Workflow We write a meta-model once and generate code stubs for the individual languages using code generators . The developer fills in the stubs manually. This allows us to scale fast between languages. Additionally, when the specification of the asset administration shell changes, the meta-model allows us to react much faster. Meta-model The meta-model is written in RDF SHACL . The meta-model specifies: * Data Structures and Operations , * Contracts of the operations, and * De/serialization scripts . Schemas such as JSON Schema and XML Schema Definition are not powerful enough as we can not specify the operations and corresopnding contracts. The meta-model needs to allow for something akin to traits , mixins or multiple inheritance so that common structures are re-used. For example, Identifiable is one such trait which is used throughout the meta-model. Code generators We develop a code generator for each programming language separately. A code generator takes the meta-model as input and generates the stub of the library: * A code generator generates the interfaces and data structures based on the meta-model. * A code generator transpiles the contracts as well as [JSON de/serialization scripts] to the corresponding implementation language. The developer needs to fill in the stubs to finalize the library. Include Markers The generated stubs contain include markers which specify parts of the code that need to be filled in. A filler script is provided to replace the include markers with the corresponding code snippets. Each include marker is globally uniquely identified. TODO (mristin, 2021-03-26): Draw a diagram here meta-model \ud83e\udc12 code generator \ud83e\udc12 stubs \ud83e\udc12 filler (with snippets as inputs); DRAW ON PAPER FIRST! ( mristin 2021-03-26: It remains to be seen if we need a filler script per language, or one for the whole library. ) Versioning The base name is aasx-core . We use the schema: aasx-core{AAS version}-{language version}-{optional runtime version} For example: * aasx-core2-java15 is the aasx-core library supporting the specification of the asset administration shell version 2 and Java 15. * Analogously, aasx-core3-csharp8-dotnet5 is the aasx-core library supporting the specification of the asset administration shell version 3, C# version 8 and the runtime .NET 5. We follow Semantic Versioning for the versioning of the library. The version X.Y.Z indicates: X is the major version (backward-incompatible), Y is the minor version (backward-compatible), and Z is the patch version (backward-compatible bug fix). Here is an example of a full identifier: aasx-core5-python38 1.43.4 .","title":"General Design Decisions"},{"location":"general-design-decisions/#general-design-decisions","text":"","title":"General Design Decisions"},{"location":"general-design-decisions/#mission","text":"We want to provide a library for manipulating the static information of the asset administration shell with a unified interface and implementations in many programming languages.","title":"Mission"},{"location":"general-design-decisions/#scope","text":"The library covers the data model of the shell and its serialization. We aim to implement the library at the level of quality ready for production . We embrace stability and long-term support instead of experimental features . The library should be minimalistic (as opposed to large enterprise systems). The library should provide base for a rich toolkit built on top of it. For example, generators such as aasx-to-grpc or aasx-to-opc-ua-server.","title":"Scope"},{"location":"general-design-decisions/#out-of-scope","text":"The component manager , the \"active\" part of the shell, is out-of-scope. This is the responsability of other components such as aasx-server . The thread-safe operations and ACID transactions are out-of-scope. The locking as well as write-ahead logs are left to the user. A different library, aasx-db (yet to be created), will provide such functionality. We expect the data model of an asset administration shell to fit in memory . Otherwise, components such as aasx-server and aasx-db (yet to be created) should be used. Since query language has not been standardized yet, we will not implement it at this point (March 2021). The library should provide basic quering through dedicated operations based on the existing implementations, such as the data model of the AASX Package Explorer .","title":"Out-of-Scope"},{"location":"general-design-decisions/#requirements","text":"The library should be provided in many programming languages . We aim for unified interface shared between the languages. Thus the users which use multiple languages can use the library effortlessly between them, as the interface remains familiar. The main user base are the developers working on stable production systems . However, the library should be flexible enough to allow experimenters to use it in their experimental code bases. Certain patterns should be supported out-of-the-box. For example, code for visitors and transformers should be included. The library should minimize the dependencies to avoid the dependency hell and be runnable on as many systems as possible. The implementations should be in native code (instead of wrapping, say, C++ code). This allows language-specific power features such as reflection. The library should provide read/write operations on AASX files . This means that blobs and packaged files should be readable/writable, but no additional features on the blobs should be made available. The library should provide import/export for JSON and XML . Other formats such as AutomationML and RDF are not supported out-of-the-box. However, adding imports and exports for additional formats should be straightforward, so that users can develop their own import/export libraries on the top. We should explicitly track how much friction there is if the user wants to use RDF, AML or any other serialization format. For example, can the user just load an AASX and use RDF? Is there a hook to set up so that the user can choose which serialization methods should be supported? The generated code should allow for static analysis as much as possible. For example, Nullable types should be used wherever appropriate. We should support wide range of computer architectures . For example, we need to support Mono in C# so that the library can run on ARM32 machines. This is important as embedded systems often run on such machines. We need to support the Environment (multiple shells) , but not the shell repository. You should be able to implement the repository using our library, but our library does not care about the repository. We postpone the discussion about the middlewares . For example, it would be practical if we allowed for a middleware to resolve ecl@ss automatically. However, this is not trivial and we leave this out at the moment (2021-03-26). The user should not use constructors to create objects. We want to keep migrations between aasx-core and aasx-db (its thread-safe pendant based on transactions) as effortless as possible. Since aasx-db is an object-oriented database , it needs to keep track of the objects and their state. Therefore, constructors need to be handled through factory methods to prepare for a future migration to aasx-db. Computational efficiency is important. The library will be used in large batches as well as servers which need to have large throughput.","title":"Requirements"},{"location":"general-design-decisions/#development-workflow","text":"We write a meta-model once and generate code stubs for the individual languages using code generators . The developer fills in the stubs manually. This allows us to scale fast between languages. Additionally, when the specification of the asset administration shell changes, the meta-model allows us to react much faster.","title":"Development Workflow"},{"location":"general-design-decisions/#meta-model","text":"The meta-model is written in RDF SHACL . The meta-model specifies: * Data Structures and Operations , * Contracts of the operations, and * De/serialization scripts . Schemas such as JSON Schema and XML Schema Definition are not powerful enough as we can not specify the operations and corresopnding contracts. The meta-model needs to allow for something akin to traits , mixins or multiple inheritance so that common structures are re-used. For example, Identifiable is one such trait which is used throughout the meta-model.","title":"Meta-model"},{"location":"general-design-decisions/#code-generators","text":"We develop a code generator for each programming language separately. A code generator takes the meta-model as input and generates the stub of the library: * A code generator generates the interfaces and data structures based on the meta-model. * A code generator transpiles the contracts as well as [JSON de/serialization scripts] to the corresponding implementation language. The developer needs to fill in the stubs to finalize the library.","title":"Code generators"},{"location":"general-design-decisions/#include-markers","text":"The generated stubs contain include markers which specify parts of the code that need to be filled in. A filler script is provided to replace the include markers with the corresponding code snippets. Each include marker is globally uniquely identified. TODO (mristin, 2021-03-26): Draw a diagram here meta-model \ud83e\udc12 code generator \ud83e\udc12 stubs \ud83e\udc12 filler (with snippets as inputs); DRAW ON PAPER FIRST! ( mristin 2021-03-26: It remains to be seen if we need a filler script per language, or one for the whole library. )","title":"Include Markers"},{"location":"general-design-decisions/#versioning","text":"The base name is aasx-core . We use the schema: aasx-core{AAS version}-{language version}-{optional runtime version} For example: * aasx-core2-java15 is the aasx-core library supporting the specification of the asset administration shell version 2 and Java 15. * Analogously, aasx-core3-csharp8-dotnet5 is the aasx-core library supporting the specification of the asset administration shell version 3, C# version 8 and the runtime .NET 5. We follow Semantic Versioning for the versioning of the library. The version X.Y.Z indicates: X is the major version (backward-incompatible), Y is the minor version (backward-compatible), and Z is the patch version (backward-compatible bug fix). Here is an example of a full identifier: aasx-core5-python38 1.43.4 .","title":"Versioning"},{"location":"naming-conventions/","text":"Naming Conventions All identifiers should use snake-case. The abbreviations should be in upper case, all other letters in lower case. We need this particular casing so that we can adapt the casing and variables to individual languages. For example, Python uses snake_case for properties, but CamelCase for classes. Golang style guide expect abbreviations to be upper case ( ParseXML ), while C# style guide wants camel case ( ParseXml ).","title":"Naming Conventions"},{"location":"naming-conventions/#naming-conventions","text":"All identifiers should use snake-case. The abbreviations should be in upper case, all other letters in lower case. We need this particular casing so that we can adapt the casing and variables to individual languages. For example, Python uses snake_case for properties, but CamelCase for classes. Golang style guide expect abbreviations to be upper case ( ParseXML ), while C# style guide wants camel case ( ParseXml ).","title":"Naming Conventions"},{"location":"roadmap/","text":"Roadmap The short-term goals (say April/May 2020) are: Focus at first only on interfaces. Only generate stubs together with contracts. Support Identifiable and a minimal support for nesting. Submodel , SubmodelElementCollection , SubmodelElement and Property Support file objects and blobs Priority of Supported Languages C#, so that the library can be already used by, say, aasx-server . Python, so that we test how it works with a slightly different paradigm. Java, so that enterprise people can use it. C++, so that embedded people can use it. C, so that even \"niche\" embedded people can use it. Golang, so that \"server\" people can use it. IEC611311, so that machine developers can use it. Erlang, so that \"network\" people can use it. Rust, for the people who use it as an alternative to C/C++.","title":"Roadmap"},{"location":"roadmap/#roadmap","text":"The short-term goals (say April/May 2020) are: Focus at first only on interfaces. Only generate stubs together with contracts. Support Identifiable and a minimal support for nesting. Submodel , SubmodelElementCollection , SubmodelElement and Property Support file objects and blobs","title":"Roadmap"},{"location":"roadmap/#priority-of-supported-languages","text":"C#, so that the library can be already used by, say, aasx-server . Python, so that we test how it works with a slightly different paradigm. Java, so that enterprise people can use it. C++, so that embedded people can use it. C, so that even \"niche\" embedded people can use it. Golang, so that \"server\" people can use it. IEC611311, so that machine developers can use it. Erlang, so that \"network\" people can use it. Rust, for the people who use it as an alternative to C/C++.","title":"Priority of Supported Languages"},{"location":"testing/","text":"Testing Test for Identity To establish the correctness of the de-serialization, contracts and serialization, we test for identity. The serialization of the de-serialization of an input should give you the very same input. Formally: serialize(deserialize(input)) == input All valid test cases should satisfy this property. All invalid test cases should report one or more errors in the test. Manual Test Cases We collect different test cases as JSON and XML files. The test cases include multiple examples of: * File represents a valid shell, * File is invalid according to the schema, * File is valid according to the schema, but represents an invalid shell. We also need special test cases to account for bugs related to how JSON handles numbers . Generated Test Cases Manually constructing shells is tedious and we will almost certainly miss many edge cases. Therefore we turn to automatic generation of test cases to increase the test coverage. Valid We script the generation of valid cases in Python by using Hypothesis library and serialize the generated shells to JSON and XML (using our library). Invalid w.r.t. Schema We randomly erase required parts or flip the types of the JSON schema. Using such a defect JSON schema we generate the invalid data automatically by sampling with hypothesis-jsonschema . This generates the cases which are invalid with respect to schema. Invalid w.r.t. Meta-model Automatically generating cases which are valid according to the schema, but invalid w.r.t. meta-model is hard as we can never be sure of the correctness of our de-serialization and contracts. Therefore we generate JSON data using hypothesis-jsonschema and manually inspect the failing examples. This will help us debug our de-serialization and contracts a bit. Since inspecting the passing examples is too laborious, we consciously miss all false positives (invalid shell, but passed de-serialization and contracts). We hope that manually constructed test cases make up for this deficiency in automatically generated cases. Test Sets Depending on the maturity of the change, we use different test sets: Small , on every commit . A selected suit of manual test cases + 100 auto-generated ones. Medium , on every alpha release. All manual test cases + 1000 auto-generated ones. Large , on every release. All manual test cases + 1,000,000 auto-generated ones. The auto-generated tests are generated only once and \"frozen\".","title":"Testing"},{"location":"testing/#testing","text":"","title":"Testing"},{"location":"testing/#test-for-identity","text":"To establish the correctness of the de-serialization, contracts and serialization, we test for identity. The serialization of the de-serialization of an input should give you the very same input. Formally: serialize(deserialize(input)) == input All valid test cases should satisfy this property. All invalid test cases should report one or more errors in the test.","title":"Test for Identity"},{"location":"testing/#manual-test-cases","text":"We collect different test cases as JSON and XML files. The test cases include multiple examples of: * File represents a valid shell, * File is invalid according to the schema, * File is valid according to the schema, but represents an invalid shell. We also need special test cases to account for bugs related to how JSON handles numbers .","title":"Manual Test Cases"},{"location":"testing/#generated-test-cases","text":"Manually constructing shells is tedious and we will almost certainly miss many edge cases. Therefore we turn to automatic generation of test cases to increase the test coverage.","title":"Generated Test Cases"},{"location":"testing/#valid","text":"We script the generation of valid cases in Python by using Hypothesis library and serialize the generated shells to JSON and XML (using our library).","title":"Valid"},{"location":"testing/#invalid-wrt-schema","text":"We randomly erase required parts or flip the types of the JSON schema. Using such a defect JSON schema we generate the invalid data automatically by sampling with hypothesis-jsonschema . This generates the cases which are invalid with respect to schema.","title":"Invalid w.r.t. Schema"},{"location":"testing/#invalid-wrt-meta-model","text":"Automatically generating cases which are valid according to the schema, but invalid w.r.t. meta-model is hard as we can never be sure of the correctness of our de-serialization and contracts. Therefore we generate JSON data using hypothesis-jsonschema and manually inspect the failing examples. This will help us debug our de-serialization and contracts a bit. Since inspecting the passing examples is too laborious, we consciously miss all false positives (invalid shell, but passed de-serialization and contracts). We hope that manually constructed test cases make up for this deficiency in automatically generated cases.","title":"Invalid w.r.t. Meta-model"},{"location":"testing/#test-sets","text":"Depending on the maturity of the change, we use different test sets: Small , on every commit . A selected suit of manual test cases + 100 auto-generated ones. Medium , on every alpha release. All manual test cases + 1000 auto-generated ones. Large , on every release. All manual test cases + 1,000,000 auto-generated ones. The auto-generated tests are generated only once and \"frozen\".","title":"Test Sets"}]}