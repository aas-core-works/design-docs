{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Design Docs for aas-core # This collection of documents captures the design decisions we made while discussing and designing the aas-core library. A Note about Validity # Their first and foremost aim is to kick-start the development and guide the initial implementation. The documents will not be continuously updated once we start with the development. However, they are still useful for the future developers! For example, parts of the code might be clearer when the initial design decisions are considered.","title":"Home"},{"location":"#design-docs-for-aas-core","text":"This collection of documents captures the design decisions we made while discussing and designing the aas-core library.","title":"Design Docs for aas-core"},{"location":"#a-note-about-validity","text":"Their first and foremost aim is to kick-start the development and guide the initial implementation. The documents will not be continuously updated once we start with the development. However, they are still useful for the future developers! For example, parts of the code might be clearer when the initial design decisions are considered.","title":"A Note about Validity"},{"location":"contracts/","text":"Code Contracts # The specification of operations in our meta-model should include the code contracts such as pre-conditions and post-conditions. The contracts can be trivial (such as specifying a range of a property), as well as very complex expressions (the length of this list needs to be divisble by three and the first element needs to be larger than the sum of all the other elements). The code generators transpile contracts so that they are included in the implementation . This has the following benefits: * The implementation of the library allows thus for deeper testing as contracts are checked during debuging, testing and in production. Additionally, the implementation can be analyzed with static analyzers and automatic testers (such as crosshair and icontract-hypothesis ), which can only work with code with tight-enough contracts. The contracts are included in the documentation of the implementation . Hence the users can rely on formal unambiguous documentation. Moreover, the documentation in human language tends to \"rot\", while contracts are continuously and automatically verified in testing or in production. Definition of Contracts # TODO (Marko & Nico & Robert, 2021-03-28): Discuss The contracts are written in our simplified Python and transpiled by code generators into the implementation stubs. As we could not support invariants in most languages, we decide to define contracts as pre-conditions and post-conditions of operations . The contracts are defined as function decorators on the opeartions: @require( condition=lambda x, y: x < y, identifier=\"x_smaller_than_y\", error=lambda x, y: \"x (%d) must be smaller than y (%d).\" % ( x, y), severity=ALWAYS, error_type=ValueError) @ensure( condition=lambda x, y, result: result > x * y, identifier=\"result_greater_than_product\", error=lambda x, y, result: (\"result (%d) must be greater than the product (%d)\" \"of x (%d) and y (%d)\") % (result, x * y, x, y), severity=DEBUG, error_type=AssertionError ) def some_operation(x: int, y: int) -> int: ... The contract decorators ( require and ensure ) as well as constants (such as ALWAYS ) live in aas_core_gen.contracts module. Identifier # Each contract must have an identifier unique to the operation. In implementation languages where we can not produce an error message, we will at least display the identifier to the user. If the identifier is omitted, it equals the body of the condition lambda function. Error Messages # The contracts should also include an error message . The error message should support printf format strings so that we can include the actual offending values in the message as well. This is important for contracts which are enforced in production catching rare and hard-to-reproduce bugs . If the error message is omitted, the code generator will try to infer the format string based on the types of the arguments of the contract condition. Error Type # While some languages, such as Golang, panic on contract violation, other languages such as Python and C# allow us to distinguish between the types of errors on contract violation. We represent the error types by using subclasses of Python Exception : ValueError for violations where the caller supplied invalid arguments. This error is mapped to System.InvalidArgumentError in C# or std::invalid_argument in C++. AssertionError for logical errors ( e.g. , operation called on the invalid state of the object). This error is mapped to System.InvalidOperationException in C# and std::logic_error in C++. The default is AssertionError . Severity # Each contract is given a severity which determines when it is enforced: ALWAYS , always enforced, including in production, DEBUG , enforced both in our and external tests, and DEBUG_OURS , enforced only in our tests, but excluded in external tests, DEBUG_SLOW and DEBUG_OURS_SLOW , enforced only in the respective tests for which we know that the amount data is small. This allows us to enforce contracts with exploding computational complexity. The default for pre-conditions is ALWAYS , while it is DEBUG for post-conditions. Contracts and Serialization # The contracts are a tool for improving the correctness of the code. They are not meant to be used as validation of the input ( e.g. , a JSON file to be imported). In our setting, this means that the validation of the input is part of the deserialization scripts . They need to check that the input is correct and conforms to the meta-model, and report the errors eventually to the user. In contrast, the contracts give us a certain assurance that the de-serialized objects are internally consistent and conform to our meta-model. In other words, the deserialization scripts verify the external data , while contracts verify the internal data . For example, we can use contracts to ensure that our deserialization works. Instead of writing tons of unit tests, we rely on the contracts to be verified during testing. A test boils down to simply de-serializing the example JSONs.","title":"Code Contracts"},{"location":"contracts/#code-contracts","text":"The specification of operations in our meta-model should include the code contracts such as pre-conditions and post-conditions. The contracts can be trivial (such as specifying a range of a property), as well as very complex expressions (the length of this list needs to be divisble by three and the first element needs to be larger than the sum of all the other elements). The code generators transpile contracts so that they are included in the implementation . This has the following benefits: * The implementation of the library allows thus for deeper testing as contracts are checked during debuging, testing and in production. Additionally, the implementation can be analyzed with static analyzers and automatic testers (such as crosshair and icontract-hypothesis ), which can only work with code with tight-enough contracts. The contracts are included in the documentation of the implementation . Hence the users can rely on formal unambiguous documentation. Moreover, the documentation in human language tends to \"rot\", while contracts are continuously and automatically verified in testing or in production.","title":"Code Contracts"},{"location":"contracts/#definition-of-contracts","text":"TODO (Marko & Nico & Robert, 2021-03-28): Discuss The contracts are written in our simplified Python and transpiled by code generators into the implementation stubs. As we could not support invariants in most languages, we decide to define contracts as pre-conditions and post-conditions of operations . The contracts are defined as function decorators on the opeartions: @require( condition=lambda x, y: x < y, identifier=\"x_smaller_than_y\", error=lambda x, y: \"x (%d) must be smaller than y (%d).\" % ( x, y), severity=ALWAYS, error_type=ValueError) @ensure( condition=lambda x, y, result: result > x * y, identifier=\"result_greater_than_product\", error=lambda x, y, result: (\"result (%d) must be greater than the product (%d)\" \"of x (%d) and y (%d)\") % (result, x * y, x, y), severity=DEBUG, error_type=AssertionError ) def some_operation(x: int, y: int) -> int: ... The contract decorators ( require and ensure ) as well as constants (such as ALWAYS ) live in aas_core_gen.contracts module.","title":"Definition of Contracts"},{"location":"contracts/#identifier","text":"Each contract must have an identifier unique to the operation. In implementation languages where we can not produce an error message, we will at least display the identifier to the user. If the identifier is omitted, it equals the body of the condition lambda function.","title":"Identifier"},{"location":"contracts/#error-messages","text":"The contracts should also include an error message . The error message should support printf format strings so that we can include the actual offending values in the message as well. This is important for contracts which are enforced in production catching rare and hard-to-reproduce bugs . If the error message is omitted, the code generator will try to infer the format string based on the types of the arguments of the contract condition.","title":"Error Messages"},{"location":"contracts/#error-type","text":"While some languages, such as Golang, panic on contract violation, other languages such as Python and C# allow us to distinguish between the types of errors on contract violation. We represent the error types by using subclasses of Python Exception : ValueError for violations where the caller supplied invalid arguments. This error is mapped to System.InvalidArgumentError in C# or std::invalid_argument in C++. AssertionError for logical errors ( e.g. , operation called on the invalid state of the object). This error is mapped to System.InvalidOperationException in C# and std::logic_error in C++. The default is AssertionError .","title":"Error Type"},{"location":"contracts/#severity","text":"Each contract is given a severity which determines when it is enforced: ALWAYS , always enforced, including in production, DEBUG , enforced both in our and external tests, and DEBUG_OURS , enforced only in our tests, but excluded in external tests, DEBUG_SLOW and DEBUG_OURS_SLOW , enforced only in the respective tests for which we know that the amount data is small. This allows us to enforce contracts with exploding computational complexity. The default for pre-conditions is ALWAYS , while it is DEBUG for post-conditions.","title":"Severity"},{"location":"contracts/#contracts-and-serialization","text":"The contracts are a tool for improving the correctness of the code. They are not meant to be used as validation of the input ( e.g. , a JSON file to be imported). In our setting, this means that the validation of the input is part of the deserialization scripts . They need to check that the input is correct and conforms to the meta-model, and report the errors eventually to the user. In contrast, the contracts give us a certain assurance that the de-serialized objects are internally consistent and conform to our meta-model. In other words, the deserialization scripts verify the external data , while contracts verify the internal data . For example, we can use contracts to ensure that our deserialization works. Instead of writing tons of unit tests, we rely on the contracts to be verified during testing. A test boils down to simply de-serializing the example JSONs.","title":"Contracts and Serialization"},{"location":"data-structures-and-operations/","text":"Data Structures and Operations # TODO (Marko & Nico & Robert, 2021-03-28): Discuss We specify the data structures and operations using our simplified Python . Based on this specification, we generate the data structures and operation stubs in the respective implementations of the library (see general design decisions ). Bootstrapping # We provide a bootstrapping script that converts the \"official\" RDF schema into our simplified Python . Since schema changes rarely, we will use diff to keep our data structures and operations in sync with the \"official\" RDF schema . Operation scripting # Many operations, such as simple add/remove commands, are trivial to script and transpile into the implementation. For such operations, we script them in our simplified Python directly in the meta-model. Thus their code is automatically generated, and needs not to be written manually in the implementations. The complex operations which cannot be scripted are marked with include markers . State # We define the state of the objects by using [dataclasses]. Both private and public state is defined, if possible. This makes the debugging and reading the implementation code across the implementation languages easier, as the reader is already familiar with the names. Private properties are marked with an underscore prefix ( _some_private_property ). If we can not meaningfully define some properties ( e.g. , since it uses a specialized data structure), we write include markers as placeholders. The developer needs to define them manually in a separate code snippet written in the corresponding implementation language. Contracts # Contracts are defined along the operations with function decorators . Please see the document Contracts for more information.","title":"Data Structures and Operations"},{"location":"data-structures-and-operations/#data-structures-and-operations","text":"TODO (Marko & Nico & Robert, 2021-03-28): Discuss We specify the data structures and operations using our simplified Python . Based on this specification, we generate the data structures and operation stubs in the respective implementations of the library (see general design decisions ).","title":"Data Structures and Operations"},{"location":"data-structures-and-operations/#bootstrapping","text":"We provide a bootstrapping script that converts the \"official\" RDF schema into our simplified Python . Since schema changes rarely, we will use diff to keep our data structures and operations in sync with the \"official\" RDF schema .","title":"Bootstrapping"},{"location":"data-structures-and-operations/#operation-scripting","text":"Many operations, such as simple add/remove commands, are trivial to script and transpile into the implementation. For such operations, we script them in our simplified Python directly in the meta-model. Thus their code is automatically generated, and needs not to be written manually in the implementations. The complex operations which cannot be scripted are marked with include markers .","title":"Operation scripting"},{"location":"data-structures-and-operations/#state","text":"We define the state of the objects by using [dataclasses]. Both private and public state is defined, if possible. This makes the debugging and reading the implementation code across the implementation languages easier, as the reader is already familiar with the names. Private properties are marked with an underscore prefix ( _some_private_property ). If we can not meaningfully define some properties ( e.g. , since it uses a specialized data structure), we write include markers as placeholders. The developer needs to define them manually in a separate code snippet written in the corresponding implementation language.","title":"State"},{"location":"data-structures-and-operations/#contracts","text":"Contracts are defined along the operations with function decorators . Please see the document Contracts for more information.","title":"Contracts"},{"location":"deserialization-scripts/","text":"De/serialization Scripts # For many similar languages following the procedural paradigm, it is tedious to write the deserialization over and over again. For example, once we implemented the deserialization of JSON in C#, it will be most probably the same thing in Java, and very similar in Golang. ( 2021-03-26: Functional languages are an exception here, but that is a problem we abstract away at the moment. ) Instead, we propose to specify the de-serialization and serialization in our simplified Python as a de/serialization script . A de/serialization script is then transpiled by the code generators into the respective implementation of the library. Operations # Each de-serialization and serialization operation is given as a function . The function operates on a jsonable , the corresponding object to which the parsed data should be added and a path to the JSON-able. For example parse_submodel_element : def parse_submodel_element( jsonable: object, submodel: Submodel, path: pathlib.Path) -> None: ... Provided Commands and Queries # It is assumed that the de/serialization script can access the operations and the data structures of our library (such as Submodel in the aforementioned example). Additionally, special commands and queries such as is_int32 , is_int64 , \u2026, as_int32 , as_int64 , as_float32 , as_float64 , as_list and as_map are provided to implement the logic. Functions such as x = local_int32(0) are used to instantiate local variables. These special commands and queries are implemented in a separate module, say, aas_core_gen.de_serialization . Error Messages # The errors should be reported using pre-defined functions error(message: str, path: pathlib.Path) . Relation to JSON Schema and XML Schema Definition # We should write a separate code generator to bootstrap this scripting code based on a schema ( e.g. , a JSON Schema ), but the schema should not be referenced in the meta-model nor available directly in code. There are two reasons for this decision: Validation and parsing of the input needs complex logic . We need to support dialects or instantiate different objects based on a complex logic. This is not possible to define in a schema. The generated implementation does not depend on a schema. Few languages support JSON Schema and XML Schema Definition out-of-box. The schema libraries, if available, tend to be slow and buggy (in general). When the \"official\" schema changes, we need to manually update our code. For example, we could use the bootstrapping code generator to re-generate the code based on the new schema and inspect the changes with the diff tool. This is clearly tedious, but we found no easy way around it. Implementation Considerations # Error logging # Error logging is more involved than it seems at first. We probably need to cap the maximum number of errors \u2014 once the error log is full, the parsing should be immediately stopped. This can be implemented by passing an ErrorLog to parse_* functions. As error logs are capped, the generated code should automatically check for available capacity at certain points (e.g., before descending into parsing the child elements), and abort the parsing if the capacity has been reached. This might also have implications on efficiency \u2014 but so do exceptions and stack unwinding! We might consider making two variants of parse_* functions \u2014 one with ErrorLog and one without (which raises the exceptions). The latter, which raises the exceptions, uses the first variant and passes an ErrorRaiser (or ErrorThrower , depending on the language) as ErrorLog . Bugs with Floats and Integers # JSON specifies that all numbers (both integers and floats) are given as 64-bit floating point numbers (see this comment on a GitHub issue ). Most JSON libraries will thus handle all numbers as 64-bit floats. This can lead to hard-to-spot bugs when the files are deserialized. Here is a Python snippet to illustrate the unexpected behavior: import json # Mind the last digit before the comma! >>> json.loads('9007199254740993.0') 9007199254740992.0 Similarly, in Golang: package main import ( \"encoding/json\" \"fmt\" ) func main() { var x interface{} json.Unmarshal([]byte(\"9007199254740993\"), &x) xFloat64 := x.(float64) fmt.Printf(\"x is %f\\n\", xFloat64) } will give you: x is 9007199254740992.000000 (again, mind the last digit). While we can not fix the de-serialization, as we usually rely on the built-in library, we could at least raise an exception during the serialization. For example, if we are serializing an 64-bit integer to JSON, we need to ensure that it is less-equal 2^52 (see again this comment on a GitHub issue ). While the issue seems rare and irrelevant, that is not the case in the practice. All data structures that involve unique identifiers given as 64-bit integer numbers will exhibit the bug if they do not increment the identifiers consecutively, but generate random 64-bit identifiers. Functional languages # We probably don't need a different scripting language for purely functional languages, but we need to think hard about it. At this stage, let us cover the procedural languages, and then think about how we can generate code for functional languages, and observe only then what walls we hit. Debugging # There should be a flag in the code generators to allow for inclusion of the original scripts in the generated code as comments. Thus the generated code is easier to trace and double-check.","title":"De/serialization Scripts"},{"location":"deserialization-scripts/#deserialization-scripts","text":"For many similar languages following the procedural paradigm, it is tedious to write the deserialization over and over again. For example, once we implemented the deserialization of JSON in C#, it will be most probably the same thing in Java, and very similar in Golang. ( 2021-03-26: Functional languages are an exception here, but that is a problem we abstract away at the moment. ) Instead, we propose to specify the de-serialization and serialization in our simplified Python as a de/serialization script . A de/serialization script is then transpiled by the code generators into the respective implementation of the library.","title":"De/serialization Scripts"},{"location":"deserialization-scripts/#operations","text":"Each de-serialization and serialization operation is given as a function . The function operates on a jsonable , the corresponding object to which the parsed data should be added and a path to the JSON-able. For example parse_submodel_element : def parse_submodel_element( jsonable: object, submodel: Submodel, path: pathlib.Path) -> None: ...","title":"Operations"},{"location":"deserialization-scripts/#provided-commands-and-queries","text":"It is assumed that the de/serialization script can access the operations and the data structures of our library (such as Submodel in the aforementioned example). Additionally, special commands and queries such as is_int32 , is_int64 , \u2026, as_int32 , as_int64 , as_float32 , as_float64 , as_list and as_map are provided to implement the logic. Functions such as x = local_int32(0) are used to instantiate local variables. These special commands and queries are implemented in a separate module, say, aas_core_gen.de_serialization .","title":"Provided Commands and Queries"},{"location":"deserialization-scripts/#error-messages","text":"The errors should be reported using pre-defined functions error(message: str, path: pathlib.Path) .","title":"Error Messages"},{"location":"deserialization-scripts/#relation-to-json-schema-and-xml-schema-definition","text":"We should write a separate code generator to bootstrap this scripting code based on a schema ( e.g. , a JSON Schema ), but the schema should not be referenced in the meta-model nor available directly in code. There are two reasons for this decision: Validation and parsing of the input needs complex logic . We need to support dialects or instantiate different objects based on a complex logic. This is not possible to define in a schema. The generated implementation does not depend on a schema. Few languages support JSON Schema and XML Schema Definition out-of-box. The schema libraries, if available, tend to be slow and buggy (in general). When the \"official\" schema changes, we need to manually update our code. For example, we could use the bootstrapping code generator to re-generate the code based on the new schema and inspect the changes with the diff tool. This is clearly tedious, but we found no easy way around it.","title":"Relation to JSON Schema and XML Schema Definition"},{"location":"deserialization-scripts/#implementation-considerations","text":"","title":"Implementation Considerations"},{"location":"deserialization-scripts/#error-logging","text":"Error logging is more involved than it seems at first. We probably need to cap the maximum number of errors \u2014 once the error log is full, the parsing should be immediately stopped. This can be implemented by passing an ErrorLog to parse_* functions. As error logs are capped, the generated code should automatically check for available capacity at certain points (e.g., before descending into parsing the child elements), and abort the parsing if the capacity has been reached. This might also have implications on efficiency \u2014 but so do exceptions and stack unwinding! We might consider making two variants of parse_* functions \u2014 one with ErrorLog and one without (which raises the exceptions). The latter, which raises the exceptions, uses the first variant and passes an ErrorRaiser (or ErrorThrower , depending on the language) as ErrorLog .","title":"Error logging"},{"location":"deserialization-scripts/#bugs-with-floats-and-integers","text":"JSON specifies that all numbers (both integers and floats) are given as 64-bit floating point numbers (see this comment on a GitHub issue ). Most JSON libraries will thus handle all numbers as 64-bit floats. This can lead to hard-to-spot bugs when the files are deserialized. Here is a Python snippet to illustrate the unexpected behavior: import json # Mind the last digit before the comma! >>> json.loads('9007199254740993.0') 9007199254740992.0 Similarly, in Golang: package main import ( \"encoding/json\" \"fmt\" ) func main() { var x interface{} json.Unmarshal([]byte(\"9007199254740993\"), &x) xFloat64 := x.(float64) fmt.Printf(\"x is %f\\n\", xFloat64) } will give you: x is 9007199254740992.000000 (again, mind the last digit). While we can not fix the de-serialization, as we usually rely on the built-in library, we could at least raise an exception during the serialization. For example, if we are serializing an 64-bit integer to JSON, we need to ensure that it is less-equal 2^52 (see again this comment on a GitHub issue ). While the issue seems rare and irrelevant, that is not the case in the practice. All data structures that involve unique identifiers given as 64-bit integer numbers will exhibit the bug if they do not increment the identifiers consecutively, but generate random 64-bit identifiers.","title":"Bugs with Floats and Integers"},{"location":"deserialization-scripts/#functional-languages","text":"We probably don't need a different scripting language for purely functional languages, but we need to think hard about it. At this stage, let us cover the procedural languages, and then think about how we can generate code for functional languages, and observe only then what walls we hit.","title":"Functional languages"},{"location":"deserialization-scripts/#debugging","text":"There should be a flag in the code generators to allow for inclusion of the original scripts in the generated code as comments. Thus the generated code is easier to trace and double-check.","title":"Debugging"},{"location":"general-design-decisions/","text":"General Design Decisions # Mission # We want to provide a library for manipulating the static information of the asset administration shell with a unified interface and implementations in many programming languages. Scope # The library covers the data model of the shell and its serialization. We aim to implement the library at the level of quality ready for production . We embrace stability and long-term support instead of experimental features . The library should be minimalistic (as opposed to large enterprise systems). The library should provide base for a rich toolkit built on top of it. For example, generators such as aas-to-grpc or aas-to-opc-ua-server. Out-of-Scope # The component manager , the \"active\" part of the shell, is out-of-scope. This is the responsibility of other components such as aasx-server . The library should not provide read/write operations on AASX files . A separate library, aas-package, should take care of that. The thread-safe operations and ACID transactions are out-of-scope. The locking as well as write-ahead logs are left to the user. A different library, aasx-db (yet to be created), will provide such functionality. We expect the data model of an asset administration shell to fit in memory . Otherwise, components such as aasx-server and aasx-db (yet to be created) should be used. Since query language has not been standardized yet, we will not implement it at this point (March 2021). The library should provide basic quering through dedicated operations based on the existing implementations, such as the data model of the AASX Package Explorer . Relation to Other Projects # Requirements # The library should be provided in many programming languages . We aim for unified interface shared between the languages. Thus the users which use multiple languages can use the library effortlessly between them, as the interface remains familiar. The main user base are the developers working on stable production systems . However, the library should be flexible enough to allow experimenters to use it in their experimental code bases. Certain patterns should be supported out-of-the-box. For example, code for visitors and transformers should be included. The library should minimize the dependencies to avoid the dependency hell and be runnable on as many systems as possible. The implementations should be in native code (instead of wrapping, say, C++ code). This allows language-specific power features such as reflection. The library should provide import/export for JSON and XML . Other formats such as AutomationML and RDF are not supported out-of-the-box. However, adding imports and exports for additional formats should be straightforward, so that users can develop their own import/export libraries on the top. We should explicitly track how much friction there is if the user wants to use RDF, AML or any other serialization format. For example, can the user just load an AASX and use RDF? Is there a hook to set up so that the user can choose which serialization methods should be supported? The generated code should allow for static analysis as much as possible. For example, Nullable types should be used wherever appropriate. We should support wide range of computer architectures . For example, we need to support Mono in C# so that the library can run on ARM32 machines. This is important as embedded systems often run on such machines. We need to support the Environment (multiple shells) , but not the shell repository. You should be able to implement the repository using our library, but our library does not care about the repository. We postpone the discussion about the middlewares . For example, it would be practical if we allowed for a middleware to resolve ecl@ss automatically. However, this is not trivial and we leave this out at the moment (2021-03-26). The user should not use constructors to create objects. We want to keep migrations between aas-core and aasx-db (its thread-safe pendant based on transactions) as effortless as possible. Since aasx-db is an object-oriented database , it needs to keep track of the objects and their state. Therefore, constructors need to be handled through factory methods to prepare for a future migration to aasx-db. Computational efficiency is important. The library will be used in large batches as well as servers which need to have large throughput. Development Workflow # We write a meta-model once and generate code stubs for the individual languages using code generators . The developer fills in the stubs manually. This allows us to scale fast between languages. Additionally, when the specification of the asset administration shell changes, the meta-model allows us to react much faster. Meta-model # The meta-model is a collection of documents written in different languages (such as RDF SHACL or our reduced version of Python, depending on the nature of the respective document). The meta-model specifies: Data Structures and Operations , Contracts of the operations, De/serialization scripts , and Unit test scripts . Schemas such as JSON Schema and XML Schema Definition are not powerful enough as we can not specify the operations and corresopnding contracts. The meta-model needs to allow for something akin to traits , mixins or multiple inheritance so that common structures are re-used. For example, Identifiable is one such trait which is used throughout the meta-model. Code generators # We develop a code generator for each programming language separately. A code generator takes the meta-model as input and generates the stub of the library: A code generator generates the interfaces and data structures based on the meta-model. A code generator transpiles the contracts as well as [JSON de/serialization scripts] to the corresponding implementation language. The developer needs to fill in the stubs to finalize the library. Include Markers # The generated stubs contain include markers which specify parts of the code that need to be filled in. A filler script is provided to replace the include markers with the corresponding code snippets. Each include marker is globally uniquely identified. ( mristin 2021-03-26: It remains to be seen if we need a filler script per language, or one for the whole library. ) Versioning # The base name is aas-core . We use the schema: aas-core{AAS version}-{language version}-{optional runtime version} For example: aas-core2-java15 is the aas-core library supporting the specification of the asset administration shell version 2 and Java 15. Analogously, aas-core3-csharp8-dotnet5 is the aas-core library supporting the specification of the asset administration shell version 3, C# version 8 and the runtime .NET 5. We follow Semantic Versioning for the versioning of the library. The version X.Y.Z indicates: X is the major version (backward-incompatible), Y is the minor version (backward-compatible), and Z is the patch version (backward-compatible bug fix). Here is an example of a full identifier: aas-core5-python38 1.43.4 .","title":"General Design Decisions"},{"location":"general-design-decisions/#general-design-decisions","text":"","title":"General Design Decisions"},{"location":"general-design-decisions/#mission","text":"We want to provide a library for manipulating the static information of the asset administration shell with a unified interface and implementations in many programming languages.","title":"Mission"},{"location":"general-design-decisions/#scope","text":"The library covers the data model of the shell and its serialization. We aim to implement the library at the level of quality ready for production . We embrace stability and long-term support instead of experimental features . The library should be minimalistic (as opposed to large enterprise systems). The library should provide base for a rich toolkit built on top of it. For example, generators such as aas-to-grpc or aas-to-opc-ua-server.","title":"Scope"},{"location":"general-design-decisions/#out-of-scope","text":"The component manager , the \"active\" part of the shell, is out-of-scope. This is the responsibility of other components such as aasx-server . The library should not provide read/write operations on AASX files . A separate library, aas-package, should take care of that. The thread-safe operations and ACID transactions are out-of-scope. The locking as well as write-ahead logs are left to the user. A different library, aasx-db (yet to be created), will provide such functionality. We expect the data model of an asset administration shell to fit in memory . Otherwise, components such as aasx-server and aasx-db (yet to be created) should be used. Since query language has not been standardized yet, we will not implement it at this point (March 2021). The library should provide basic quering through dedicated operations based on the existing implementations, such as the data model of the AASX Package Explorer .","title":"Out-of-Scope"},{"location":"general-design-decisions/#relation-to-other-projects","text":"","title":"Relation to Other Projects"},{"location":"general-design-decisions/#requirements","text":"The library should be provided in many programming languages . We aim for unified interface shared between the languages. Thus the users which use multiple languages can use the library effortlessly between them, as the interface remains familiar. The main user base are the developers working on stable production systems . However, the library should be flexible enough to allow experimenters to use it in their experimental code bases. Certain patterns should be supported out-of-the-box. For example, code for visitors and transformers should be included. The library should minimize the dependencies to avoid the dependency hell and be runnable on as many systems as possible. The implementations should be in native code (instead of wrapping, say, C++ code). This allows language-specific power features such as reflection. The library should provide import/export for JSON and XML . Other formats such as AutomationML and RDF are not supported out-of-the-box. However, adding imports and exports for additional formats should be straightforward, so that users can develop their own import/export libraries on the top. We should explicitly track how much friction there is if the user wants to use RDF, AML or any other serialization format. For example, can the user just load an AASX and use RDF? Is there a hook to set up so that the user can choose which serialization methods should be supported? The generated code should allow for static analysis as much as possible. For example, Nullable types should be used wherever appropriate. We should support wide range of computer architectures . For example, we need to support Mono in C# so that the library can run on ARM32 machines. This is important as embedded systems often run on such machines. We need to support the Environment (multiple shells) , but not the shell repository. You should be able to implement the repository using our library, but our library does not care about the repository. We postpone the discussion about the middlewares . For example, it would be practical if we allowed for a middleware to resolve ecl@ss automatically. However, this is not trivial and we leave this out at the moment (2021-03-26). The user should not use constructors to create objects. We want to keep migrations between aas-core and aasx-db (its thread-safe pendant based on transactions) as effortless as possible. Since aasx-db is an object-oriented database , it needs to keep track of the objects and their state. Therefore, constructors need to be handled through factory methods to prepare for a future migration to aasx-db. Computational efficiency is important. The library will be used in large batches as well as servers which need to have large throughput.","title":"Requirements"},{"location":"general-design-decisions/#development-workflow","text":"We write a meta-model once and generate code stubs for the individual languages using code generators . The developer fills in the stubs manually. This allows us to scale fast between languages. Additionally, when the specification of the asset administration shell changes, the meta-model allows us to react much faster.","title":"Development Workflow"},{"location":"general-design-decisions/#meta-model","text":"The meta-model is a collection of documents written in different languages (such as RDF SHACL or our reduced version of Python, depending on the nature of the respective document). The meta-model specifies: Data Structures and Operations , Contracts of the operations, De/serialization scripts , and Unit test scripts . Schemas such as JSON Schema and XML Schema Definition are not powerful enough as we can not specify the operations and corresopnding contracts. The meta-model needs to allow for something akin to traits , mixins or multiple inheritance so that common structures are re-used. For example, Identifiable is one such trait which is used throughout the meta-model.","title":"Meta-model"},{"location":"general-design-decisions/#code-generators","text":"We develop a code generator for each programming language separately. A code generator takes the meta-model as input and generates the stub of the library: A code generator generates the interfaces and data structures based on the meta-model. A code generator transpiles the contracts as well as [JSON de/serialization scripts] to the corresponding implementation language. The developer needs to fill in the stubs to finalize the library.","title":"Code generators"},{"location":"general-design-decisions/#include-markers","text":"The generated stubs contain include markers which specify parts of the code that need to be filled in. A filler script is provided to replace the include markers with the corresponding code snippets. Each include marker is globally uniquely identified. ( mristin 2021-03-26: It remains to be seen if we need a filler script per language, or one for the whole library. )","title":"Include Markers"},{"location":"general-design-decisions/#versioning","text":"The base name is aas-core . We use the schema: aas-core{AAS version}-{language version}-{optional runtime version} For example: aas-core2-java15 is the aas-core library supporting the specification of the asset administration shell version 2 and Java 15. Analogously, aas-core3-csharp8-dotnet5 is the aas-core library supporting the specification of the asset administration shell version 3, C# version 8 and the runtime .NET 5. We follow Semantic Versioning for the versioning of the library. The version X.Y.Z indicates: X is the major version (backward-incompatible), Y is the minor version (backward-compatible), and Z is the patch version (backward-compatible bug fix). Here is an example of a full identifier: aas-core5-python38 1.43.4 .","title":"Versioning"},{"location":"naming-conventions/","text":"Naming Conventions # All identifiers should use snake-case. The abbreviations should be in upper case, all other letters in lower case. We need this particular casing so that we can adapt the casing and variables to individual languages. For example, Python uses snake_case for properties, but CamelCase for classes. Golang style guide expect abbreviations to be upper case ( ParseXML ), while C# style guide wants camel case ( ParseXml ).","title":"Naming Conventions"},{"location":"naming-conventions/#naming-conventions","text":"All identifiers should use snake-case. The abbreviations should be in upper case, all other letters in lower case. We need this particular casing so that we can adapt the casing and variables to individual languages. For example, Python uses snake_case for properties, but CamelCase for classes. Golang style guide expect abbreviations to be upper case ( ParseXML ), while C# style guide wants camel case ( ParseXml ).","title":"Naming Conventions"},{"location":"roadmap/","text":"Roadmap # The short-term goals (say April/May 2020) are: Focus at first only on interfaces. Only generate stubs together with contracts. Support Identifiable and a minimal support for nesting. Submodel , SubmodelElementCollection , SubmodelElement and Property Support file objects and blobs Priority of Supported Languages # C#, so that the library can be already used by, say, aasx-server . Python, so that we test how it works with a slightly different paradigm. Java, so that enterprise people can use it. C++, so that embedded people can use it. C, so that even \"niche\" embedded people can use it. Golang, so that \"server\" people can use it. IEC611311, so that machine developers can use it. Erlang, so that \"network\" people can use it. Rust, for the people who use it as an alternative to C/C++.","title":"Roadmap"},{"location":"roadmap/#roadmap","text":"The short-term goals (say April/May 2020) are: Focus at first only on interfaces. Only generate stubs together with contracts. Support Identifiable and a minimal support for nesting. Submodel , SubmodelElementCollection , SubmodelElement and Property Support file objects and blobs","title":"Roadmap"},{"location":"roadmap/#priority-of-supported-languages","text":"C#, so that the library can be already used by, say, aasx-server . Python, so that we test how it works with a slightly different paradigm. Java, so that enterprise people can use it. C++, so that embedded people can use it. C, so that even \"niche\" embedded people can use it. Golang, so that \"server\" people can use it. IEC611311, so that machine developers can use it. Erlang, so that \"network\" people can use it. Rust, for the people who use it as an alternative to C/C++.","title":"Priority of Supported Languages"},{"location":"simplified-python/","text":"Simplified Python # We use a simplified version of Python as the universal language to be transpiled into the respective implementation languages (such as C#, Java etc. ). This simple Python supports only a certain subset of language constructs and built-in functions. The language is used for defining data structures and operations , contracts and scripting de/serialization . As the scripts are not used by the wide audience, but only by the developers of the library, we intentionally decide on-the-go which Python features to support. Hence we only transpile a very limited subset of builtin functions and types. Disallowed Features # We allow no: Nested functions, Context managers, and Custom intermediate classes ( e.g. , classes which are not directly transpiled and used by the user such as data structures ) String Formatting # To facilitate the transpilation to languages such as C++ and Golang, we decide to use old string formatting : >>> '%s has %03d quote types.' % (\"Python\", 2)) 'Python has 002 quote types.' Supported Language Constructs # For-loops # For-loops can loop either over lists, over maps or over for _ in range(..., ...) where both start and end are given as integers. Include Markers # Since we only provide a transpilation based on a subset of Python language, we will have to include manually defined code snippets at many spots. The developer should mark such spots with include markers using NotImplementedError . For example: raise NotImplementedError('some-marker') will make the code generators introduce an include marker some-marker in the generated code. The filler script will eventually replace the include markers with the corresponding code snippets written in the implementation language.","title":"Simplified Python"},{"location":"simplified-python/#simplified-python","text":"We use a simplified version of Python as the universal language to be transpiled into the respective implementation languages (such as C#, Java etc. ). This simple Python supports only a certain subset of language constructs and built-in functions. The language is used for defining data structures and operations , contracts and scripting de/serialization . As the scripts are not used by the wide audience, but only by the developers of the library, we intentionally decide on-the-go which Python features to support. Hence we only transpile a very limited subset of builtin functions and types.","title":"Simplified Python"},{"location":"simplified-python/#disallowed-features","text":"We allow no: Nested functions, Context managers, and Custom intermediate classes ( e.g. , classes which are not directly transpiled and used by the user such as data structures )","title":"Disallowed Features"},{"location":"simplified-python/#string-formatting","text":"To facilitate the transpilation to languages such as C++ and Golang, we decide to use old string formatting : >>> '%s has %03d quote types.' % (\"Python\", 2)) 'Python has 002 quote types.'","title":"String Formatting"},{"location":"simplified-python/#supported-language-constructs","text":"","title":"Supported Language Constructs"},{"location":"simplified-python/#for-loops","text":"For-loops can loop either over lists, over maps or over for _ in range(..., ...) where both start and end are given as integers.","title":"For-loops"},{"location":"simplified-python/#include-markers","text":"Since we only provide a transpilation based on a subset of Python language, we will have to include manually defined code snippets at many spots. The developer should mark such spots with include markers using NotImplementedError . For example: raise NotImplementedError('some-marker') will make the code generators introduce an include marker some-marker in the generated code. The filler script will eventually replace the include markers with the corresponding code snippets written in the implementation language.","title":"Include Markers"},{"location":"testing/","text":"Testing # We perform two kinds of testing: Unit tests , which test the individual operations, and Identity tests , which test the de/serialization. Scripted Unit Tests # Since we develop in a polyglot setting, the logic of most of the unit tests will be the same across the languages. Therefore we script those unit tests in a common language. Similar to de/serialization scripts , we generate the implementations in the respective languages based on these unit test scripts. We will initially use a subset of Python. We do not know at this point (2021-03-27) which language features we need nor what constructs we need to support. It is also possible that we can script tests using even a simpler language ( e.g. , a chain of JSON objects describing the actions). Hence it remains to be further discussed how this scripting of unit tests should be implemented. Identity Tests # To establish the correctness of the de-serialization, contracts and serialization, we test for identity. The serialization of the de-serialization of an input should give you the very same input. Formally: serialize(deserialize(input)) == input All valid test cases should satisfy this property. All invalid test cases should report one or more errors in the test. Manual Test Cases # We collect different test cases as JSON and XML files. The test cases include multiple examples of: File represents a valid shell, File is invalid according to the schema, File is valid according to the schema, but represents an invalid shell. We also need special test cases to account for bugs related to how JSON handles numbers . Generated Test Cases # Manually constructing shells is tedious and we will almost certainly miss many edge cases. Therefore we turn to automatic generation of test cases to increase the test coverage. Valid # We script the generation of valid cases in Python by using Hypothesis library and serialize the generated shells to JSON and XML (using our library). Invalid w.r.t. Schema # We randomly erase required parts or flip the types of the JSON schema. Using such a defect JSON schema we generate the invalid data automatically by sampling with hypothesis-jsonschema . This generates the cases which are invalid with respect to schema. Invalid w.r.t. Meta-model # Automatically generating cases which are valid according to the schema, but invalid w.r.t. meta-model is hard as we can never be sure of the correctness of our de-serialization and contracts. Therefore we generate JSON data using hypothesis-jsonschema and manually inspect the failing examples. This will help us debug our de-serialization and contracts a bit. Since inspecting the passing examples is too laborious, we consciously miss all false positives (invalid shell, but passed de-serialization and contracts). We hope that manually constructed test cases make up for this deficiency in automatically generated cases. Test Sets # Depending on the maturity of the change, we use different test sets: Small , on every commit . A selected suit of manual test cases + 100 auto-generated ones. Medium , on every alpha release . All manual test cases + 1000 auto-generated ones. Large , on every release . All manual test cases + 1,000,000 auto-generated ones. The auto-generated tests are generated only once and \"frozen\".","title":"Testing"},{"location":"testing/#testing","text":"We perform two kinds of testing: Unit tests , which test the individual operations, and Identity tests , which test the de/serialization.","title":"Testing"},{"location":"testing/#scripted-unit-tests","text":"Since we develop in a polyglot setting, the logic of most of the unit tests will be the same across the languages. Therefore we script those unit tests in a common language. Similar to de/serialization scripts , we generate the implementations in the respective languages based on these unit test scripts. We will initially use a subset of Python. We do not know at this point (2021-03-27) which language features we need nor what constructs we need to support. It is also possible that we can script tests using even a simpler language ( e.g. , a chain of JSON objects describing the actions). Hence it remains to be further discussed how this scripting of unit tests should be implemented.","title":"Scripted Unit Tests"},{"location":"testing/#identity-tests","text":"To establish the correctness of the de-serialization, contracts and serialization, we test for identity. The serialization of the de-serialization of an input should give you the very same input. Formally: serialize(deserialize(input)) == input All valid test cases should satisfy this property. All invalid test cases should report one or more errors in the test.","title":"Identity Tests"},{"location":"testing/#manual-test-cases","text":"We collect different test cases as JSON and XML files. The test cases include multiple examples of: File represents a valid shell, File is invalid according to the schema, File is valid according to the schema, but represents an invalid shell. We also need special test cases to account for bugs related to how JSON handles numbers .","title":"Manual Test Cases"},{"location":"testing/#generated-test-cases","text":"Manually constructing shells is tedious and we will almost certainly miss many edge cases. Therefore we turn to automatic generation of test cases to increase the test coverage.","title":"Generated Test Cases"},{"location":"testing/#valid","text":"We script the generation of valid cases in Python by using Hypothesis library and serialize the generated shells to JSON and XML (using our library).","title":"Valid"},{"location":"testing/#invalid-wrt-schema","text":"We randomly erase required parts or flip the types of the JSON schema. Using such a defect JSON schema we generate the invalid data automatically by sampling with hypothesis-jsonschema . This generates the cases which are invalid with respect to schema.","title":"Invalid w.r.t. Schema"},{"location":"testing/#invalid-wrt-meta-model","text":"Automatically generating cases which are valid according to the schema, but invalid w.r.t. meta-model is hard as we can never be sure of the correctness of our de-serialization and contracts. Therefore we generate JSON data using hypothesis-jsonschema and manually inspect the failing examples. This will help us debug our de-serialization and contracts a bit. Since inspecting the passing examples is too laborious, we consciously miss all false positives (invalid shell, but passed de-serialization and contracts). We hope that manually constructed test cases make up for this deficiency in automatically generated cases.","title":"Invalid w.r.t. Meta-model"},{"location":"testing/#test-sets","text":"Depending on the maturity of the change, we use different test sets: Small , on every commit . A selected suit of manual test cases + 100 auto-generated ones. Medium , on every alpha release . All manual test cases + 1000 auto-generated ones. Large , on every release . All manual test cases + 1,000,000 auto-generated ones. The auto-generated tests are generated only once and \"frozen\".","title":"Test Sets"}]}